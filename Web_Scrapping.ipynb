{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f8fbea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#import readability\n",
    "from itertools import zip_longest\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e3e4f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "articles_text =[]\n",
    "authors = []\n",
    "articles_url = []\n",
    "links=[]\n",
    "sep=[]\n",
    "foo =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "40844572",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/'\n",
    "#requests html page to parse\n",
    "page = requests.get(url)\n",
    "#view page content\n",
    "src = page.content\n",
    "#parses page and stores it in the 'soup' variable\n",
    "soup = BeautifulSoup(src, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "03ca1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#searches for headline in the HTML\n",
    "headline = soup.findAll(\"a\", {\"class\":\"block-link__overlay-link\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "3497073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(headline)):\n",
    "    headlines.append(headline[i].text)\n",
    "    links.append(headline[i]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5868bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the \"/n & space from the list of the headlines\"\n",
    "for i in range(len(headlines)):\n",
    "    temp = headlines[i]\n",
    "    headlines[i]=temp[2:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8a340e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # add the 'https://www.bbc.com' to the links\n",
    "    for link in links:\n",
    "        if link[0]=='/':\n",
    "            articles_url.append('https://www.bbc.com'+link)\n",
    "        else:\n",
    "            articles_url.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e12975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in articles_url:\n",
    "    result = ''.join([z for z in i if not z.isdigit()])\n",
    "    ahm = result.replace(\"-\",\"\")\n",
    "    foo.append(ahm.lstrip(\"https://www.bbc.com/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f8420b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(foo)):\n",
    "    sep.append(foo[i].split('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5e8a5fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for every_link in articles_url:\n",
    "    page = requests.get(every_link)\n",
    "    src = page.content\n",
    "    soup = BeautifulSoup(src, 'html.parser')\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "2a36032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.findAll(\"article\", {\"class\":[\"qa-post gs-u-pb-alt+ lx-stream-post gs-u-pt-alt+ gs-u-align-left\"]}):\n",
    "    title = soup.findAll(\"h2\",{\"class\":\"ssrcss-1s5ma9r-StyledHeading e1fj1fc10\"})\n",
    "    author = soup.findAll(\"p\", {\"class\":[\"lx-commentary__meta-reporter gel-long-primer\",\"ssrcss-1gg9z89-Contributor e5xb54n2\",\"qa-contributor-name lx-stream-post__contributor-name gel-long-primer gs-u-m0\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a03f1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in range(len(title)):\n",
    "    titles.append(title[i].text)\n",
    "for z in author:\n",
    "    authors.append(z.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "674bb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [headlines,articles_url,sep,title,author]\n",
    "exported = zip_longest(*file_list)\n",
    "with open(\"D:/web scraping/The_data.csv\",\"w\")as myfile:\n",
    "    wr =csv.writer(myfile)\n",
    "    wr.writerow([\"Headlines\",\"Main_url\",\"Category\",\"Sup_titles\",\"Author\"])\n",
    "    wr.writerows(exported)\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
